{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Necessary Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install -q -U pip   # [optional] upgrade pip to the latest version\n",
    "!pip install pandas==2.2.2 -q -U   # install pandas for handling CSV files\n",
    "!pip install openai==1.43.0 -q -U  # install OpenAI library for generating question-answer pairs\n",
    "!pip install datasets==3.2.0 -q -U # install Hugging Face datasets library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Restart Kernel to Activate Installed Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# Define a function to display a progress bar during synthetic data generation\n",
    "# e.g. [██████████████████████████████--------------------] 60.67%\n",
    "def print_progress(cur_data, total_data):\n",
    "    total_bar = 50  # set total size of the progress bar to be 50\n",
    "    cur_percent = (cur_data / total_data) * 100\n",
    "    cur_bar = int((cur_data / total_data) * total_bar)\n",
    "    cur_bar_display = '█' * cur_bar + '-' * (total_bar - cur_bar)\n",
    "    # for intermediate print, always start from linehead using \\r and avoid moving to the next line using end=''\n",
    "    print(f'\\r[{cur_bar_display}] {cur_percent:.2f}%', end='')\n",
    "\n",
    "# Define a function to calculate the cost of API calls based on the number of tokens used\n",
    "def calculate_token_cost(model, input_token_count=0, output_token_count=0):\n",
    "    input_token_price = 0\n",
    "    output_token_price = 0\n",
    "    \n",
    "    # define token prices (dollars per million tokens, price as of Jan 2025) based on the model\n",
    "    if model == 'gpt-4o':\n",
    "        input_token_price = 2.5\n",
    "        output_token_price = 10.0\n",
    "    elif model == 'gpt-4o-mini':\n",
    "        input_token_price = 0.15\n",
    "        output_token_price = 0.6\n",
    "\n",
    "    # calculate the total cost of API calls\n",
    "    cost = (input_token_count / 1_000_000 * input_token_price +\n",
    "            output_token_count / 1_000_000 * output_token_price)\n",
    "    return round(cost, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert 'Raw Dataset' (raw_csv, containing exhibits' raw descriptions) into 'Synthetic Dataset' (synthetic_csv, containing exhibits-related question-answer pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to convert one exhibit's raw description into 15 question-answer pairs using OpenAI API\n",
    "#\n",
    "# Format of raw exhibit description:\n",
    "#   \"Object Details Physical Description Canard biplane with one 12-horsepower Wright horizontal four-cylinder engine driving two pusher propellers ...\"\n",
    "#\n",
    "# Format of generated 15 question-answer pairs:\n",
    "#   \"[Visitor]: Can you describe this exhibit?  [Guide]: This exhibit showcases the remarkable 1903 Wright Flyer ...\n",
    "#    [Visitor]: What can you tell me about this exhibit?  [Guide]: The exhibit features the iconic 1903 Wright Flyer ...\n",
    "#    ...\n",
    "#    [Visitor]: Can you summarize the significance of this exhibit?  [Guide]: The exhibit features the 1903 Wright Flyer, a groundbreaking ...\n",
    "#   \"\n",
    "def generate_synthetic_data(raw_description):\n",
    "    client = OpenAI(api_key=\"your OpenAI API key\")  # replace with your OpenAI API key\n",
    "    questions_per_description_per_temperature = 5\n",
    "\n",
    "    system_prompt = f'''\n",
    "You are an expert in creating high-quality museum exhibit descriptions. You will be provided with a detailed description of an exhibition below, delimited with XML tags. Your task is to generate {questions_per_description_per_temperature} different descriptions that convey the same core information but are rewritten with varied wording, structure, and style. The new descriptions should be as informative and accurate as the original but should appear unique in phrasing to serve as synthetic data for fine-tuning a large language model. Ensure that the technical details are preserved and that each description is clear and concise, suitable for inclusion in a museum's official records. Each description should be in one paragraph and no more than 300 words.\n",
    "\n",
    "In addition, prepend each description with a varied question that inquires about the exhibition, such as 'What can you tell me about this exhibit?', 'Can you describe this exhibit?', 'Could you provide an overview of what is on display here?', or other similar phrasings, assuming the visitor has no prior knowledge about the exhibition, so do not include the provided exhibition info in the question, to make it more like a visitor-guide chat.\n",
    "\n",
    "Print out your answer in the following format: [Visitor]: Visitor's question\\n[Guide]: Guide's description\\n\n",
    "    '''\n",
    "    user_prompt = f\"<exhibition>{raw_description}</exhibition>\"\n",
    "\n",
    "    temperature_list = [0.4, 0.7, 0.9]\n",
    "    result = \"\"\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "\n",
    "    for temperature in temperature_list:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "                max_tokens=3000,\n",
    "                top_p=1\n",
    "            )\n",
    "            response_content = completion.choices[0].message.content\n",
    "            result += response_content + '\\n'\n",
    "            # accumulate token usage for cost calculation\n",
    "            input_tokens += completion.usage.prompt_tokens\n",
    "            output_tokens += completion.usage.completion_tokens\n",
    "        except Exception as e:\n",
    "            print(f\"OpenAI API call failed at temperature: {temperature}, Error: {e}\")\n",
    "            \n",
    "    return result, input_tokens, output_tokens\n",
    "\n",
    "# Define a function to convert 'Raw Dataset' (raw_csv, containing all exhibits' raw descriptions) into 'Synthetic Dataset' (synthetic_csv, containing all exhibits' question-answer pairs)\n",
    "#\n",
    "# Format of raw_csv:\n",
    "# |item_id|item_name        |link                           |train_images                       |validation_images|test_images     |description                                             |\n",
    "# |0      |1903-wright-flyer|https://...1903-wright-flyer...|\"1903-wright-flyer-1.jpg  a.jpg ..\"|\"b.jpg c.jpg ..\" |\"d.jpg e.jpg ..\"|\"Object Details Physical Description Canard biplane ...\"|\n",
    "# ...\n",
    "#\n",
    "# Format of synthetic_csv:\n",
    "# |item_id|item_name        |train_images                       |validation_images|test_images     |synthetic_dialogs                                                                          |\n",
    "# |0      |1903-wright-flyer|\"1903-wright-flyer-1.jpg  a.jpg ..\"|\"b.jpg c.jpg ..\" |\"d.jpg e.jpg ..\"|\"[Visitor]: Q1? [Guide]: A1. [Visitor]: Q2? [Guide]: A2. ... [Visitor]: Q15? [Guide]: A15.\"|\n",
    "# ...\n",
    "def generate_synthetic_csv(raw_csv, synthetic_csv):\n",
    "    raw_df = pd.read_csv(raw_csv)\n",
    "    syn_df = raw_df[['item_id', 'item_name', 'train_images', 'validation_images', 'test_images']].copy()\n",
    "    syn_df['synthetic_dialogs'] = None\n",
    "\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "\n",
    "    for idx, row in raw_df.iterrows():\n",
    "        # generate synthetic question-answer pairs from the raw description\n",
    "        valid_response = False\n",
    "        while not valid_response:\n",
    "            generated_text, input_tokens, output_tokens = generate_synthetic_data(row['description'])\n",
    "            if generated_text.count('[Visitor]') == 15 and generated_text.count('[Guide]') == 15: \n",
    "                valid_response = True\n",
    "            else:\n",
    "                print(f\"\\nInvalid response, retrying item: {row['item_name']} ...\")\n",
    "        syn_df.at[idx, 'synthetic_dialogs'] = generated_text.replace(\"’\", \"'\")  # clean up formatting issues\n",
    "\n",
    "        # display progress and the total cost of the API calls\n",
    "        print_progress(idx + 1, len(raw_df))\n",
    "        total_input_tokens += input_tokens\n",
    "        total_output_tokens += output_tokens\n",
    "        total_cost = calculate_token_cost('gpt-4o-mini', total_input_tokens, total_output_tokens)\n",
    "        print(f' [gpt-4o-mini] total_tokens: {total_input_tokens}(input), {total_output_tokens}(output), total_cost: ${total_cost}', end='')\n",
    "\n",
    "    # save the synthetic dataset to the specified CSV file\n",
    "    syn_df.to_csv(synthetic_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[██████████████████████████████████████████████████] 100.00% [gpt-4o-mini] total_tokens: 136209(input), 216127(output), total_cost: $0.15"
     ]
    }
   ],
   "source": [
    "# Convert 'Raw Dataset' into 'Synthetic Dataset'\n",
    "generate_synthetic_csv(\"./smithsonian_raw_data.csv\", \"./smithsonian_synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Convert 'Synthetic Dataset' (synthetic_csv, containing exhibits' question-answer pairs) into 'Synthetic Dataset Splits' (train_csv, validation_csv, test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to apply Idefics2-style formatting on raw question-answer pair text\n",
    "#\n",
    "# Format of text_in:\n",
    "#   [Visitor]: Can you describe this exhibit? [Guide]: This exhibit showcases xxx\n",
    "#\n",
    "# Format of text_out:\n",
    "#   {\"messages\": [\n",
    "#                   {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Can you describe this exhibit?\"} ]}, \n",
    "#                   {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"This exhibit showcases xxx\"} ]} \n",
    "#                ]\n",
    "#   }\n",
    "import re\n",
    "def apply_idefict2_style(text_in):\n",
    "    text_seg = re.split(r'\\[Visitor\\]:|\\[Guide\\]:', text_in)\n",
    "    text_out = '{\"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"'\n",
    "    text_out += text_seg[1].replace('\"', '\\\\\"').strip()\n",
    "    text_out += '\"} ]}, {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": \"'\n",
    "    text_out += text_seg[2].replace('\"', '\\\\\"').strip()\n",
    "    text_out += '\"} ]} ]}'\n",
    "    return text_out\n",
    "\n",
    "# Define a function to compose a 'Synthetic Dataset Split' from 'Synthetic Dataset' (synthetic_csv)\n",
    "#\n",
    "# Format of synthetic_csv:\n",
    "# |item_id|item_name        |train_images                       |validation_images|test_images     |synthetic_dialogs                                                                          |\n",
    "# |0      |1903-wright-flyer|\"1903-wright-flyer-1.jpg  a.jpg ..\"|\"b.jpg c.jpg ..\" |\"d.jpg e.jpg ..\"|\"[Visitor]: Q1? [Guide]: A1. [Visitor]: Q2? [Guide]: A2. ... [Visitor]: Q15? [Guide]: A15.\"| \n",
    "# ...\n",
    "#\n",
    "# Format of dataset-split (e.g. validation_csv):\n",
    "# |data_id|item_name        |image_name             |file_name              |dialog                                                                                    |\n",
    "# |0      |1903-wright-flyer|b.jpg                  |b.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q10?}, {\"role\": \"assistant\", \"content\": A10.}] }|\n",
    "# |1      |1903-wright-flyer|c.jpg                  |c.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q11?}, {\"role\": \"assistant\", \"content\": A11.}] }|\n",
    "# ...\n",
    "def add_data_in_df(df, item, img_list, dialog_list):\n",
    "    existing_data_count = len(df)\n",
    "    if len(img_list) != len(dialog_list):\n",
    "        exit(f\"Error: unmatched image count ({len(img_list)}) and dialog count ({len(dialog_list)}) when updating {item}\")\n",
    "    new_rows = []\n",
    "    for i in range(len(dialog_list)):\n",
    "        new_rows.append({\n",
    "            'data_id': existing_data_count + i,\n",
    "            'item_name': item,\n",
    "            'image_name': img_list[i],\n",
    "            'file_name': img_list[i],\n",
    "            'dialog': apply_idefict2_style(dialog_list[i])\n",
    "        })\n",
    "    return pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# Define a function to partition 'Synthetic Dataset' (synthetic_csv) into 'Synthetic Dataset Splits' (train_csv, validation_csv, test_csv)\n",
    "#\n",
    "# Format of synthetic_csv:\n",
    "# |item_id|item_name        |train_images                       |validation_images|test_images     |synthetic_dialogs                                                                          |\n",
    "# |0      |1903-wright-flyer|\"1903-wright-flyer-1.jpg  a.jpg ..\"|\"b.jpg c.jpg ..\" |\"d.jpg e.jpg ..\"|\"[Visitor]: Q1? [Guide]: A1. [Visitor]: Q2? [Guide]: A2. ... [Visitor]: Q15? [Guide]: A15.\"|\n",
    "# ...\n",
    "#\n",
    "# Format of dataset-split (train_csv):\n",
    "# |data_id|item_name        |image_name             |file_name              |dialog                                                                                  |\n",
    "# |0      |1903-wright-flyer|1903-wright-flyer-1.jpg|1903-wright-flyer-1.jpg|{\"messages\": [{\"role\": \"user\", \"content\": Q1?}, {\"role\": \"assistant\", \"content\": A1.}] }|\n",
    "# |1      |1903-wright-flyer|a.jpg                  |a.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q2?}, {\"role\": \"assistant\", \"content\": A2.}] }|\n",
    "# ...\n",
    "#\n",
    "# Format of dataset-split (validation_csv):\n",
    "# |data_id|item_name        |image_name             |file_name              |dialog                                                                                    |\n",
    "# |0      |1903-wright-flyer|b.jpg                  |b.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q10?}, {\"role\": \"assistant\", \"content\": A10.}] }|\n",
    "# |1      |1903-wright-flyer|c.jpg                  |c.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q11?}, {\"role\": \"assistant\", \"content\": A11.}] }|\n",
    "# ...\n",
    "#\n",
    "# Format of dataset-split (test_csv):\n",
    "# |data_id|item_name        |image_name             |file_name              |dialog                                                                                    |\n",
    "# |0      |1903-wright-flyer|d.jpg                  |d.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q13?}, {\"role\": \"assistant\", \"content\": A13.}] }|\n",
    "# |1      |1903-wright-flyer|e.jpg                  |e.jpg                  |{\"messages\": [{\"role\": \"user\", \"content\": Q14?}, {\"role\": \"assistant\", \"content\": A14.}] }|\n",
    "# ...\n",
    "def generate_dataset(synthetic_csv, train_csv, validation_csv, test_csv):\n",
    "    syn_df = pd.read_csv(synthetic_csv)\n",
    "    # 'file_name' is a reserved column name in Hugging Face used to link image data\n",
    "    # https://huggingface.co/docs/datasets/main/en/image_dataset#imagefolder\n",
    "    test_df = pd.DataFrame(columns=['data_id', 'item_name', 'image_name', 'file_name', 'dialog'])\n",
    "    valid_df = pd.DataFrame(columns=['data_id', 'item_name', 'image_name', 'file_name', 'dialog'])\n",
    "    train_df = pd.DataFrame(columns=['data_id', 'item_name', 'image_name', 'file_name', 'dialog'])\n",
    "\n",
    "    for idx, row in syn_df.iterrows():\n",
    "        dialog_list = row['synthetic_dialogs'].split('[Visitor]')\n",
    "        dialog_list = ['[Visitor]' + elem.strip() for elem in dialog_list if elem.strip()]\n",
    "\n",
    "        test_img_list = [elem.strip() for elem in row['test_images'].split('\\n') if elem.strip()]\n",
    "        valid_img_list = [elem.strip() for elem in row['validation_images'].split('\\n') if elem.strip()]\n",
    "        train_img_list = [elem.strip() for elem in row['train_images'].split('\\n') if elem.strip()]\n",
    "\n",
    "        if not test_img_list or not valid_img_list or not train_img_list:\n",
    "            print(\"Error: img_list is empty.\")\n",
    "\n",
    "        test_df = add_data_in_df(test_df, row['item_name'], test_img_list, dialog_list[0:len(test_img_list)])\n",
    "        valid_df = add_data_in_df(valid_df, row['item_name'], valid_img_list, dialog_list[len(test_img_list):len(test_img_list) + len(valid_img_list)])\n",
    "        train_df = add_data_in_df(train_df, row['item_name'], train_img_list, dialog_list[len(test_img_list) + len(valid_img_list):])\n",
    "\n",
    "    test_df.to_csv(test_csv, index=False, encoding='utf-8') \n",
    "    valid_df.to_csv(validation_csv, index=False, encoding='utf-8') \n",
    "    train_df.to_csv(train_csv, index=False, encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'Synthetic Dataset' into 'Synthetic Dataset Splits'\n",
    "generate_dataset(\"./smithsonian_synthetic_data.csv\", \"./image/train/metadata.csv\", \"./image/validation/metadata.csv\", \"./image/test/metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Hugging Face Dataset from Synthetic Dataset Splits (train_csv, validation_csv, test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# Define a function to create a local Hugging Face dataset and then upload it to Hugging Face\n",
    "# https://huggingface.co/docs/datasets/main/en/image_load#imagefolder\n",
    "# https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api#huggingface_hub.HfApi.create_repo\n",
    "# https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.push_to_hub\n",
    "def create_and_upload_hf_dataset(local_dataset_dir, repo_id):\n",
    "    # create a local Hugging Face dataset\n",
    "    dataset_hf = load_dataset(\"imagefolder\", data_dir=os.path.join(local_dataset_dir, \"image\"))\n",
    "    dataset_hf.save_to_disk(os.path.join(local_dataset_dir, \"hf_dataset\"))\n",
    "    # inspect the dataset splits ('train', 'validation', 'test')\n",
    "    print(dataset_hf)\n",
    "\n",
    "    # upload local Hugging Face dataset to Hugging Face Hub\n",
    "    HfFolder.save_token(\"your Hugging Face Access Token\")  # replace with your Hugging Face access token\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_id, repo_type=\"dataset\")\n",
    "    dataset_hf.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d9cb321bce431dac176c730c5baba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/820 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace535a2b24b4b41b6d1c18e46b0a87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e71ad2d2f64307b88d81c36bbb9dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c10d230b7cf406d88baefaa7fdd4ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/823 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bd2920833f4dc1ace24887fab4fe72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/86 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec84f6d5201c4bb9ae9fa96ad5436886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/78 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7050fb95ce344eb986b0a2faf7ac12a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabdd34e1f004005b4638c84ca45aae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77363116e324797a8a03c00241ea77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0a8769d5ec4eb896d0294e2258f91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5427b15b479a47b38f56e17d07d9955b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8502c1091a5c42dc851ecd31fcfd9449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'data_id', 'item_name', 'image_name', 'dialog'],\n",
      "        num_rows: 819\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'data_id', 'item_name', 'image_name', 'dialog'],\n",
      "        num_rows: 82\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['image', 'data_id', 'item_name', 'image_name', 'dialog'],\n",
      "        num_rows: 74\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7e903db85d4506b1778319602075e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91cf3fff48f3447ea2d1c4cbb153abe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3e6eaa65284cf8b57ca5bc70819668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8a3fff085403d95f0fca166caffb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf452110d7b14b339628da981f565415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17355d63293e4e369872e018e2292f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f1a6d966f746bdb8882b77f025c13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e55413ffbba547269da2284b03fa981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93fbdcac595404cb21a18f56ab28108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and upload the Hugging Face dataset\n",
    "create_and_upload_hf_dataset(\"./\", \"xugefu/MuseQuest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
